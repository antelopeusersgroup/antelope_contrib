.TH DBXCOR 1 "$Date$"
.SH NAME
dbxcor - GUI-based, multichannel cross-correlation program 
.SH SYNOPSIS
.nf
dbxcor db [-appname name -o dbout [-q queuefile | -i infile ] -pf pffile -V -v]
.fi
.SH DESCRIPTION
.LP
The dbxcor program is a multichannel cross-correlation program 
designed to provide a robust way to estimate arrival times for
a gather of seismograms.  The original algorithm was designed for
teleseismic body wave processing, but this version extends the capabilities
to generic gathers that are aligned by any reference frame 
that can be conceived.  The type example in seismology other than
an event gather is a receiver gather used for source array processing. 
.LP
The basic algorithm is a type of array processing approach.
As such it works only for signals that are reasonably matched
across the entire array aperture.  The program combines three
concepts:  stacking, cross-correlation with an array stack,
and robust estimators.  
That is, all correlation is done against an array stack
computed by a nonlinear (robust) stacking algorithm.
The algorithm used has been found to work
well on data with highly variable signal to noise conditions.
The algorithm, however, requires a seed of one station 
used as the starting point for the nonlinear stacking algorithm.
This is done for two reasons.  First, most large aperture arrays have
one or more exceptionally quiet stations that can fill this role.  
Second, we can't normally use any standard array stack as a starting
point as no plane wave or typical model-based travel time estimate
can align data well enough to avoid degrading the stack at short
periods.  
.LP
The robust stacking loop begins with a median stack computed from
the data aligned using the initial station selected interactively.  
It then enters the following iterative loop:
.nf
  1) do
    2) compute robust stack of data = beam
    3) correlate and align each trace with beam
  4) until (all lags are less than one sample)
.fi
.LP
This program is database driven, but has different modes of operation 
described below.  The most important restriction is that it cannot start
from nothing.  The input database must contain an event and origin table
that will be referenced in processing.  The primary outputs are 
assoc, arrival, and four extension tables (xcorarrival, xcorbeam, 
wfprocess, and evlink) used to store outputs special to this program.  
.LP
Command line arguments are:
.IP db
\fIdb\fP is the working database unless dbout is defined.
That is, normally this program reads all it's information from db
and updates and/or appends to existing tables in this database.
This parameter always defines the database from which waveforms are to be extracted.
It can contain continuous data or segmented data, but it must have
links to at least site for the program to be functional.
The details of how the input data need to be organized are described in 
more detail below this depends on the processing mode.
.IP -appname
The default behavior of this program is to drive the program through
smartpick.  In this mode the program
uses the tksend (1)
mechanism to communicate.  tksend uses the name defined by this parameter
as the target used for communication.  This defaults to dbxcor.
.IP -o
Optional output database name.  When the -o parameter is used assoc,
arrival, xcorarrival, xcorbeam, wfprocess, and evlink tables are written
to this database.  Note that normally dbxcor does database updates on
any existing arrivals tied to a particular event through assoc.   
A convenient way to avoid updates and preserve existing arrivals is to 
specify dbout as an independent database.
.IP -i
Read input from a file instead of through the tksend mechanism.  
The file name passed through this argument is assumed to contain
a set of lines with two parameters per line containing ordered
pairs of orid and phase names.  For example, 
.nf
	49 P
	49 S
	49 PP
	82 P
	82 PP
.fi
Would drive the program to process two events (orid 49 and 82) 
with a mix of P, S, and PP phases.  These lines are identical to 
the input passed through the smartpick interface, but you, the
user, in this case need to handle the event and phase description
instead of making the choices interactively..
.IP -q 
Use queuefile and the processing queue method for data handling.
In this mode the tksend mechanism is disabled and the program is 
driven by a processing queue cached in the file queuefile. If queuefile
does not exist it will be created.  The queue file is a binary file 
used to cache records to be processed.  In event processing this is
typically records in evid.  When run in this mode the program will
keep track of which events have already been processed and pick up where
you left off in a previous run.  This mode is highly recommended if
processing a large data set with a static database.  This mode
must NEVER be used on a dynamic database or results can be unpredictable.
Note also that -i and -q modes are mutually exclusive.  dbxcor will
exit if you try to define both.
This method is required if running in GenericGather mode.
.IP -pf
Is used to specify an alternative parameter file than the default of
dbxcor.pf.
.IP -V 
Show usage line and exit.
.IP -v
Run in verbose mode.
.SH Operating Modes
This program can be run in a least six common permutations.
.IP (1)
Driven by smartpick.  In this mode the idea is to use dbpick to 
explore the data and decide what teleseismic phases are worthy
of processing.  When a phase with a workable signal is present
push the appropriate button on smartpick to arm dbxcor with
data for that phase from the particular event of interest.
.IP (2)
One can avoid smartpick and drive dbxcor directly from dbpick
without smartpick as an intermediate agent.  Launch dbxcor and
dbpick on the same X display.  Find the data you want to process
an issue a command like this in the dbpick window:
.nf
   exec tksend dbxcor %orid P
.fi
where P could be changed to any teleseismic phase of interest known to 
the tau-p calculator.  
.IP (3)
The -i option can be used to process a list of events known to have workable
phases for one or more phases.  For example, one might select all teleseismic
events from 30 to 95 degrees with magnitude greater than 6 and have some confidence
all will yield workable P and S phases.  The major disadvantage of this approach
at present is that you, the user, would need to keep track of where you leave
off between runs of the program.  
.IP (4)
The -q options is the best choice for mass processing of a static database.
This approach was designed to work for both event processing and source
array processing.  In this mode the program keeps track of which records
in an input view have been processed.  This mode, however, does not allow
switching seismic phases during processing.  Hence, a given queue file 
might be run for all P phases and the data rerun with a new queue file
for S, SS, etc.  
This is also the required mode for source array processing.
.IP (5) 
The use of the -o options is highly recommended if you want to run this program on
a real-time database.  Because the back end of the program does database updates to 
arrival and assoc it is intrinsically dangerous to run the program on a dynamically 
changing database linked to a real time system.  Do be warned, however, that some 
information can be lost when using the -o option.  In normal operations dbxcor 
matches any existing arrivals and only updates the measured arrival time, residual 
field in assoc, and auth fields.  Fields like snr computed by real time pickers 
are not transferred when the program runs with the -o option.  If you need to preserve
other information in arrival and assoc the best advice as always is to work on a 
copy of the database derived from a snapshot of a real time system.  
.IP (6)
This program was designed to allow it to process source array gathers.  
This requires an input view to drive the processing created with dbprocess
(see parameter file description below) and running the program with 
generic gathers as the processing mode.  
.LP
The front end of this program is a graphical user interface using
the X Motif and X toolkit libraries.  The graphical iterface has four main sections:
.IP (1)
A top bar with pull-down menus.  The menus are File, Edit,  Picks, Option, and View.  
File is largely an alternative exit.  Edit contains a number of trace editing
functions described below.  Picks is used to select processing
time windows (see below).  The options menu provides a means to bring up
thee popup windows;  one for data sort criteria, one for filter options,
and one for multichannel correlator options.
Finally, view is used to run various attribute plots an station names on or off.
.IP (2)
A seismic data display section.  The data shown are displayed using a 
Motif widget (see seisw(3)) developed from a modification of the Seismic Unix program
called xwigb.  This display makes all three buttons on a three-button mouse
sensitive.  MB1 (left) drag is a zoom function.  MB2 (middle) is used to pick
a point or a single seismoram (e.g. reference trace - see below).  MB3 is
used to pick time windows (Pick menu items - see below).
.IP (3)
Contains a log of informational messages.  The first place to look if problems
occur is this window.
.IP (4)
A set of action buttons along the bottom row.  Normal, simple processing
of this row of buttons proceeds from left to right, but situations sometimes
alter this.  
.LP
The main processing is driven by the row of buttons in the bottom row of the GUI.
The buttons are in active state only when the action they
represent is appropriate.
Buttons are disabled until required actions are
taken.  Key states of the program the user needs to be aware of
are the following:
.IP (1)
When the program starts up it is in one of two possible startup states.
When the -f option is used a "Get Next Event" button appears and is the only
active widget in the display.  When using the tksend mechanism no elements of 
the GUI are initially enabled and the program.  This means the program is
listening for instructions from tksend and will sleep until it receives instructions.
.IP (2)
Whenever data are loaded the enable/disable subarrays toggle button is active.  `
As the name suggests this toggles the program between full array and subarray processing.
(One or more subarrays must be defined in the parameter file for this to have meaning
and the program may abort if no valid subarrays are defined.)
.IP (3)
When data is first loaded "Pick Ref. Trace" is enabled.  The user
must interactively pick a reference event (seed for robust stacking method) 
before further processing can be completed.  To pick a reference trace point
at the desired trace and click MB2 (middle button).
.IP (4)
Once a reference event is picked, the "Analyze" button is enabled.
When pushed this button initiates the primary analysis of the 
program. 
.IP (5) 
When the analysis is completed the "Plot Beam" and "Plot Correlation" 
buttons are enabled.  If the results are acceptable, the user is
next required to push the "Plot Beam" button and pick an arrival 
time (use MB2) on the beam trace.  This is necessary to resolve the otherwise
ambiguous dc arrival value problem. i.e. this pick is used to set
the position of the arrival estimates relative to a theoretical time
computed from the predicted arrival time as sum of the relative time
on the team and the lag computed by cross-correlation.
.IP (6) 
When a time is picked on the beam the "Save" button is enabled.
Only then can the results be saved to the database.
.LP
Actual processing rarely proceeds in one pass through the states 
described above.  Any of the following options are often 
necessary.  (Note that if a button is enabled, the 
processing you request is feasible.  If you encounter exceptions
to this rule please report this to the authors.)
.IP (1)
Filters other than the default are frequently necessary.  Alternative
filters can be selected from the Options->Filter Options pulldwon menu.
This launches a popup menu that will stay up until dismissed by pushing
the cancel button.  Note that filters are cumulative.  When you apply
a new filter it is applied to the data currently being displayed.  
In this way you can, or example, bandpass filter and then integrate.
The "Restore Data" button at the bottom of the display can always be used
to restore the original data filtered only with the default filter for 
the requested phase.  
.IP (2) 
Bad traces can and normally should be edited out by one of two mechanisms.
First, the Edit->Trace Edit button is a toggle.  When selected all elements of the 
GUI are disabled except the inverse toggle labeled "Stop Trace Edit" under the edit menu.  Click 
on traces to be killed with MB2.  As you do so they will be marked dead, 
made flat, and colored red.  The mark dead attribute is reversible
so if you hit the wrong trace
by mistake click it a second time with MB2 and it will be restored.  The 
second method for editing is only possible after running Analyze at least once.
Alternatively, click with MB1 on the button labeled "Pick Cutoff".  Pick a trace on the display
with MB2 and all data below that trace on the display will be marked dead.  
.IP (3)
The edit menu has a "Restore Data" button that is the ultimate escape.
It allows you to 
essentially start over as if you had just finished reading the current block 
of data.  Note these data may not be sorted in the same order as the data 
before hitting this button.
.IP (4)
The "Picks" menu item has three selections: (1) Beam window, (2) Robust Window,
and (3) Reference Trace.  
The later (Reference Trace) is an alternative interface to the "Pick Reference" 
button at the bottom of the display.  It was placed here to make the interaction
more efficient because all three entries are hit constantly and as a result
have keyboard accelerators (see below).
Experience has shown that care in selection of the two windows is critical 
for successful results with this program.  In fact, the default windows set
through the parameter file are rarely ideal and experience shows altering
these time windows is almost always advisd.  The Beam Window defines the time gate
used for cross-correlation.  The best advice in choosing it is to start a few seconds
before the earliest observed arrival time to a time a few seconds past the point where
most of the data show a common waveform.  For smaller deep events this is commonly only a few 
cycles of the phase.  In contrast, something like a shallow magnitude 8 event can 
work effectively with a beam a minute or more in length.  (Warning:  using smartpick
and dbpick can help avoid obvious blunders like accidentally mixing two phases in a
long time gate in such a situation.)  The "Robust Window" pick is equally important
for obtaining good results, especially with variable quality data.  The robust 
loss function used to weight data computed using only data in this time window.  
Experience has shown that the best results are normally obtained by defining the
robust window as the first one or two clear cycles of the phase being analyzed.  
This is well justified theoretically as the earliest part of the signal is less
prone to being modified by any form of scattering.   
You should also be aware that an amplitude scale factor is computed an applied to
each trace using data within the robust window.  This normally improves the look
of the display dramatically after an analysis and provide a useful relative amplitude
scale factor that is written to the output database.
.IP (5)
After pushing the analysis button the data are always sorted by the stack weight 
parameter.  If the time gates are chosen correctly it has been found that this 
parameter does an extremely good job of sorting data in reliability order with the 
data most like the beam at the top of the display and the data least like the beam 
(usually also the noisiest) at the bottom.  Use the "Pick Cutoff" and/or 
"Trace Edit" buttons to kill problem data.  It is generally prudent to 
then pick a new reference trace an push the Analyze button again.  
In marginal signal to noise conditions this may need to be repeated several 
times to discard all the low signal-to-noise ratio data.  
.IP (6)
In processing marginal signal-to-noise events two other strategies often 
help.  First, push the "Correlation Plot" button and examine the cross-correlation 
functions.  Poorly defined maxima in the correlation function can help you decide
if a given trace should be marked bad.  Secondly, try the Options->Sort Options menu
item.  This will bring up a choice of alternative sort criteria for traces in
the display.  You can, for example, sort by coherence or peak cross correlation
and use them as an alternative "Pick Cutoff" attribute.  
.IP (7)
Large arrays will not always have a coherent phase that can be stacked with the full
array aperture.  This is especially true for small teleseismic event P waves that
are commonly detectable only in the traditional short period band.  For this reason
dbxcor has an option menu item labeled "Enable Subarrays" (reciprocal is labeled 
"Disable Subarrays").  You can switch back and forth between full and subarray processing
BUT but be careful of two pitfalls.  First, it is inadvisable to save results
from both subarray and full array processing of the same event.  It is allowed, but
it may cause you headaches downstream.  In any case, you must be aware of
an important limitation of subarray processing.  When running in subarray mode
when you push the "Save" button arrival and assoc WILL NOT be updated.  Results
will be saved only to the extension table xcorarrival.  The reason for this is
that in subarray processing a station can and often will have multiple lag estimates.
The extreme version of this is the conventional all-pairs method which essentially
does subarray processing for all possible two-station subarrays.  The resolution of
the problem here is the same one needed for the two-station method;  a least squares
method is needed to resolve the ambiguity of multiple arrival estimates for the
same station.  A program to do this driven by the xcorarrival table is under 
development.  The second pitfall in subarray processing is that it is currently 
impossible to do anything but a linear pass through the subarray list.  That is,
once enabled the pointer for the list of subarrays is reset to the top and 
a button labeled "Next Subarray" becomes active.  
(Note there is no "Previous Subarray" button because no such feature 
is currently implemented.) A useful strategy for subarray processing
is to enable subarrays and first go through all the subarray data
by clicking the "Next Subarray" button until the list is exhausted.
The goal in this scan is to decide if using the subarray feature is likely 
to be successful for the event being analyzed.  If you decide the answer
is yes, push "Restore Data" and the subarray pointer will be reset to the
first subarray in the master list.  Then proceed through the event
by processing each subarray gather, hitting save, and then 
"Next Subarray".  When the list is exhausted the buttons will all 
become inactive.  Be aware that the primary reason a "Previous Subarray"
button does no exist is that the xcorarrival table is not handled in
a update mode.  That is, the program only blindly adds new rows to this
table when the save button is pushed.  This will always be successful 
because the xcorarrival table uses an integer id to tie the arrival to 
a particular beam trace indexed with wfprocess.  The primary consequence of
rerunning a subarray is that the least squares procedure to compute
arrival has to deal with this potential irregularity.
.IP (8)
The "View" menu allows display of one or more attributes linked to 
each seismogram in the display.  These are of two types. The sta 
button enables a station name label for each trace.  This is often 
useful for finding that super station in every network that might
be a good choice as a reference trace.  It also helps in quality
control by helping identify stations that are not consistent
with others in the array.  This can indicate an equipment failure,
error in metadata, or a real property of the earth. The second type
of plot is an x-y graph of one or more trace attributes.  Currently
this is limited to:  stack weight, coherence, and peak cross correlation.
.SH MULTICHANNEL CORRELATOR MODES
.LP
Experience with earlier versions of this program revealed a need for three
different ways of running the multichannel correlation procedure that is
the core of this program: (1) the normal mode of correlate and stack
described in the first section of this man page; (2) a "Stack Only" mode 
in which cross-correlations functions are computed but data are not allowed
to be time shifted; and (3) a "Correlate Only" mode where data are time shifted
but the stack is not computed but overwritten by the windowed reference trace.
This mode is set through the Options->MCC Options menu item.  When selected a
popup window is create with radio buttons that are used to select one of these
mutually exclusive options.
.LP
This feature was implemented because of several practical problems that arose
in initial test of this program.
.IP (1)
Sometimes one or more traces are subject to cycle skips that are impossible
to work around.  When faced with this problem use the manual picking function or
the cycle skip repair edit feature to time shift the problem trace to what you
believe is the correct time, enable the "Stack Only" MCC Option button, and run
the process again.  This will allow the problem signal to participate in the 
stack in the proper timing relationship.
.IP (2)
In array processing of teleseismic data the ideal window to correlate and stack is not
necessarily the desired window for subsequent processing.  The "Stack Only" mode 
cca be useful in this mode if you set the parameter LoadArrivals true and
set GatherTimeAlignmentKey to arrival.time.  
.IP (3)
The "Correlate Only" mode is most useful in source array processing.  A common 
receiver gather from events spread through a region of any size will contain a mix
of data that are extremly well matched, data that are similar but fairly divergent,
and data that are wildly divergent from any other components of the gather.  This
mode has been found useful to find and correlate the really similar waveforms in
an ensemble.  A useful strategy when facing a large gather of divergent signals is
to visually find events with very similar waveforms, select a relatively long 
beam time window, and run the analysis in correlate only mode.  This is a very 
effective way to find waveforms that strongly ressemble the reference as the
result will be sorted (by default) by the peak cross-correlation with the reference
trace.
.SH MANUAL PICKING FEATURES
.LP
Two different manual pick correction methods are available.  Both are accessible
from buttons on the bottom row of the GUI.  The \fIManual Picking\fP button is 
a simple picker.  Point at a trace, click MB2, and the seismogram will be shifted to 
make that point to zero in the arrival time reference frame.  Note this is only 
the same as picking the arrival time if the rest of the data are aligned relative
to the 0 mark.  Be warned that sometimes the algorithm used in the multichannel
correlation processing will shift 0 somewhat making this picking process more awkward
than would be ideal.
.LP
The second mode of picking is more elaborate.  If you push the "Magnify Picking" button
the main display is armed to produce a special picking method.  When enabled all other 
buttons on the GUI are disabled until this button is pushed again.  The primary purpose
of this method is to repair cycle skips in cross-correlation.  Point at a trace that
needs to be fixed and click on it somewhere with MB2.  This will bring up a new window
with three seismogram.  In order from the bottom up these are: (1) the seismogram 
to be picked, (2) the current beam estimate, and (3) the cross-correlation function
between the beam and the displayed trace.  You must pick a point on the cross-correlation
trace to shift the data to correct a cycle skip.  The concept to keep in mind is to point
at the peak of the correlation function that you believe is more appropriate than the 
maximum chosen by the program.  The traces in the new window and the master display 
will be updated based on the selected pick.  (Note:  the picker does not use any kind of
maximum finder.  It simply uses the time defined by your pick.).
.SH WAVEFORM DATA MODELS
.PP
This program expects waveforms assembled in different ways depending
on the setting of the "processing_mode" parameter (see below).  
.LP
When processing_mode is \fIContinuousDB\fP the program assumes the
database contains a wfdisc table that will be treated as continuous
data AND that the processing will be event-based.  
Note that the data do not absolutely have to be continuous, but the
approach is time window based.  That is, the algorithm used is to carve
out time windows defined around predicted and/or measured arrival times.
.LP
When processing_mode is set to EventGathers it is assumed the data have been
literally assembled into event gathers produced by the program extract_events.
The model used by extract_events is full three-component objects indexed by
an extension table wfprocess (also used to store beam output).  This mode
is recommended when processing a large data set as assembling the gathers
in this way has a huge impact on interactive performance.  extract_events
does significant preprocessing that would otherwise have to be done on
the fly.  The author has seen order of magnitude improvements in data 
read times with this method relative to continuousDB processing.  
.LP
When processing_mode is set to GenericGathers the entire input view is driven
by the contents of what is built through the "dbprocess_command" parameter (see
below).  This means the meaning of gather can be completely different.  In particular,
this mode can be used to implement source array processing in which bundles of 
events assembled into receiver gathers can be used to produce a set of picks
consistent with a given gather.  Note that in GenericGather processing mode 
the "Save" button behaves differently.  Because generic gathers might contain
multiple suites of event that could be correlated differently, the display 
acts recursively in this mode.  That is, when the save button is pushed data
marked live and saved to the database are deleted from the gather and the 
display is refreshed to include only the debris (i.e. data that didn't correlate
in the previous analysis).  The user may choose to move on and push the 
"Next Gather" button or try to do further processing on these remaining traces.
This process can be repeated until no data remain in the gather or until the
"Next Gather" button us pushed.  Furthermore, in this mode results are currently 
saved to a different output table called xsaa that differs somewhat from 
the xcorarrival table created in other modes. 
.SH KEYBOARD ACCELERATORS
.LP
Several the menu items use Motif keyboard accelerators.  This means typing
the stated key anywhere in the gui is equivalent to following a particular
menu chain.  Currently defined accelerators are:
.IP b
Alias for Picks->Beam Window.
.IP r 
Alias for Picks->Robust Window.
.IP m
Alias for Picks->Reference Trace.  The "m" is intended to be mnemonic for "master" trace 
with a meaning similar to reference.  The obvious choice of "r" collides with the 
accelerator or the robust window pick name so m was chosen.  The reference trace concept
was retained because earlier versions of the program and documentation use that terminology
repeatedly.  Apologies to all users confused by this.  
.IP t
Alias for Edit->Trace Edit.  This enables or disables the single trace kill (edit) function
with MB2.
.IP c
Alias for Edit->Pick Cutoff.  This enables and disables the trace cutoff picking method
described above.  (MB2 selection kills all traces below picked point)
.IP p
Alias for Edit->Enable Polarity Edit.  This enables the polarity flipping feature
with MB2 described above.
.IP d
Alias for Edit->Restore Data.  That is, it is the escape button that clears
everything and restores the original ensemble.
This is supposed to be mnemonic for "data".  Could not 
use the more logical "r" because it was already used as an accelerator
for picking the robust window. 
.SH PARAMETER FILE
.LP
The complete parameter file for this program is very long
because of the need to describe all the graphical defaults.
A large fraction of these are best left alone.  Here we 
describe only the parameters the end user will need to be aware of.
They are grouped in two sections:  (1) Must Change and (2) May
Require Changes.
.ce
\fIMust Change\fP
.fi
.LP
\fIinitial_time_stamp\fP is an estimate of the approximate start time
of the data set being analyzed.  Any time which doesn't yield an empty
site table should work.  This is necessary because dbxcor uses a dynamic
method to maintain it's station geometry table derived from ondate and
offdate in the site table.  The internally cached table has a time span 
defined by valid time ranges linked to the current time stamp.  Once 
data are loaded the time span of the previously processed event is used
but on startup a rational time is required to initialize properly.  
The best choice is some time known to be inside the ondate to offdate
range of the stations being used for this run.
.LP
\fInetwork_name\fP should be a unique word used to describe the full
array being processed.  Beam traces produced as output with the full array
will be tagged with this name.
.LP
\fIphase_processing_parameters\fP is the tag or an Arr list of parameters 
for each phase to be processed. This will be clearer with an example:
.nf
phase_processing_parameters     &Arr{
    P   &Arr{
        analysis_sort_order     stack_weight
        arrival_channel Z
        beam_window_fraction    0.6
        component_for_analysis  L
        default_filter  BW 0.01 2 2.0 2
        phase_for_analysis      P
        regular_gather_twin_end 120.0
        regular_gather_twin_start       -20.0
        robust_window_fraction  0.2
        stacking_method robust
        tpad    30.0
        channel_expression BHZ
    }
    PP  &Arr{
        analysis_sort_order     stack_weight
        arrival_channel Z
        beam_window_fraction    0.8
        component_for_analysis  Z
        default_filter  BW 0.01 2 1.0 2
        phase_for_analysis      PP
        regular_gather_twin_end 120.0
        regular_gather_twin_start       -20.0
        robust_window_fraction  0.2
        stacking_method robust
        tpad    30.0
        channel_expression ..Z.*
    }
}
.fi
.LP
This example defines how the program should handle two phases:  P and PP. 
Note that each phase defines values for the same parameter names.  
\fIanalysis_sort_order\fP defines how the data will be sorted when
processing is completed after the Analyze button is pushed.  
stack_weight is recommended, but alternatives are coherence, 
correlation_peak, amplitude, or moveout.  
\fIarrival_channel\fP is a channel code that will be used to 
tag the entry in arrival.  
Single character codes of Z, N, or E must be used for this parameter.
They are substituted based on input data for SEED channel codes derived from
the original data.  
Two other channel parameters are used in different context:
\fIchannel_expression\fP and \fIcomponent_for_analysis\fP.
Which of these two parameters is used  depends on the setting of the 
global boolean \fIRequireThreeComponents\fP.  When true 
\fIchannel_expression\fP is ignored but \fIcomponent_for_analysis\fP is
used.  When false the reverse is true.
The \fIchannel_expression\fP argument allows any arbitrary subset
condition that would be allowed as a subset expression in 
Datascope.  (e.g. .HZ.* would select a collection of 
Z channels like BHZ, LHZ, BHZ_00, and BHZ_01.)
For three component processing there
is a fundamental problem 
here that is awkward to deal with no matter what you do.  
In that mode a single character code is used that 
make sense only for a three-component data.  Allowed values
for \fIcomponent_for_analysis\fP are Z,N,E,T,R, and L.  
The program will abort on startup if you use anything else for 
this parameter.  
Z, N, and E are cardinal directions
while R, T, and L are ray coordinate directions of radial, transverse,
and longitudinal.  Note that R, T, and L are produced using the
free surface transformation of Kennett (1991).  In either case in 
three component processing the data a transformation matrix is always constructed
and data are rotated as needed.  
Because the single code channels 
\fIbeam_window_fraction, robust_window_fraction, regular_gather_twin_start\fP,
and \fIregular_window_twin_end\fP are closely linked.  The regular gather window
parameters define the range of data that will be displayed on the GUI in
the arrival time reference frame (i.e. 0 for each trace is the predicted
arrival time of the requested phase).  This time window is multiplied by
\fIbeam_window_fraction\fP and \fIrobust_window_fraction\fP to define 
initial values for the beam and robust windows.  
\fItpad\fP and \fIdefault_filter\fP should be considered together are are
used in the same way as tpad parameters in dbpick(1).  That is, \fItpad\fP
is a time padding added to the beginning and end of the regular gather window
to allow for edge transients for the filter.
Finally, \fIphase_for_analysis\fP should normally match the key for this
block of parameters.  It is required because of the laziness of the author
as it simplifies the process of parsing this already complicated pf file.
.LP
\fIprocessing_mode\fP should be one of three options:  (1) EventGathers,
(2) GenericGathers, or (3) ContinuousData (default and case used if
you make a typo in the first 2 option names).  If anything but the 
default (ContinuousData) case is set, a second parameter is required
tagged with the keyword \fIdefault_phase\fP which must name a phase defined
by the phase definitions described above.  This defines the phase
that will be used in processing in these modes.  The setting of 
\fIprocessing_mode\fP is a primary control on the kinds of data
the program expects.  
When running in ContinuousData mode (the default) data are extracted 
using a time window with the window start and end times driven by 
predicted arrival times for a specified phase.  In this mode the 
phase can change with every request for new data since the presumption
is that the program is reading from a CSS3.0 database with a wfdisc
that indexes a suite of continuous data.  
The GenericGathers and EventGathers mode, in contrast, 
require the input data to be
segmented and these modes require the segments define blocks of data 
long enough that they can be subsequently windowed into the 
range defined by the processing window parameters.
Both of these modes use the a secondary parameter tagged
\fIdbprocess_commands\fP that is a Tbl passed to dbprocess to 
build the working view used to define the gather.  The default
parameter file for this program gives forms for this Tbl that
are appropriate for EventGather and GenericGather forms.
The concept used for this mode, however, is very general and
is intended to be schema independent.  The basic idea is that
dbprocess must build a view that closes with a dbgroup 
clause that defines the definition of what a "gather" means.
For an EventGather this is restrictive.  It MUST be a group
on evid.  For a GenericGather there is no real restriction.
The primary intent for GenericGather is actually a source array
gather.  That is, a collection of event seismograms grouped by
station and some method for linking events with common waveforms.
Current experimental mode uses a cluster table as used by 
pmelgrid.
.LP
\fIresample_definitions\fP and \fItarget_sample_interval\fP are inseparably 
linked.  As the name implies \fItarget_sample_interval\fP is the sample
interval (in seconds) to which all the data will be resampled.  
The contents of the \fIreasample_definitions\fP Arr block describe the
recipes used to resample different sample rates.  That block is
also very complicated, but is described in detail in dbresample (1).
For most cases these parameters are set once and require no further
changes unless new data with a different sample rate are introduced.
.ce
\fIMay Require Changes\fP
.fi
.LP
\fIAutoscaleInitialPlot\fP controls initial plot scaling.  By default 
data are all displayed initially at true amplitude.  If this parameter is set
true, each trace in the display will be scaled to have an equal peak amplitude.
This is most useful when data are contaminated by several bad traces with 
large spikes or uncompensated offsets.  These can render a signal of interest
invisible without extensive editing. Normally it should be false unless 
the data quality is very poor.
.LP
As the name implies \fIRequireThreeComponents\fP is a boolean that tells
the program if it should be dogmatic about requiring three component data.
When true the program will automatically drop any data not having three 
components.  This parameter should always be true if you choose to 
process channels R, T, or L (ray coordinates) since these make no 
sense otherwise.  The only time this parameter would normally be set is
in processing P wave phases only and you specify using channel Z.
.LP
\fIStationChannelMap\fP is a complicated Arr used to resolve the 
ambiguities of what channel to use for stations with multiple loc 
codes or high gain and low gain channels.  The dbxcor program uses a 
general solution to this problem implemented through the StationChannelMap
processing object (see 
http://seismo.geology.indiana.edu/~pavlis/software/seispp/html/db/d13/classSEISPP_1_1StationChannelMap.html ).
This is another case where an example helps explain the idea better than 
a bunch of words:
.nf
StationChannelMap       &Arr{
    SDV &Tbl{
        BHE_00 0 0
        BHN_00 1 0
        BHZ_00 2 0
        BHE_10 0 1
        BHN_10 1 1
        BHZ_10 2 1
        HHE_10 0 2
        HHN_10 1 2
        HHZ_10 2 2
        HHE_20 0 3
        HHN_20 1 3
        HHZ_20 2 3
    }
    default     &Tbl{
        BHE 0 0
        BHN 1 0
        BHZ 2 0
        HHE 0 1
        HHN 1 1
        HHZ 2 1
        LHE 0 2
        LHN 1 2
        LHZ 2 2
    }
}
.fi
This is an example from the Bolivar experiment.  Most stations in that experiment
used B, H, and/or L channels.  The "default" section specifies the channel hierarchy for 
most of the stations in that experiment.  BHE, BHN, and BHZ have the highest precedence
(last column being 0 defines this); HHE, HHN, and HHZ have secondary precedence, and L
channels have the lowest precedence.  Precedence means that if a station has more than
one channel code the higher precedence data will be used first.  If higher precedence 
data are absent the program works down the chain to try to find alternatives and gives up
if there are no matches.  The middle column defines the component number used to place 
each channel in a standard reference frame.  Normally you should have 0 be x1, 1 be x2, 
and 2 be x3.  This is not necessarily essential as a transformation matrix is always
computed, but it is best to not tempt fate.  You must absolutely not repeat a component
number at the same precedence in this list or you the missing component will be undefined.
.LP
The above example also has a typical entry for a GSN station.  Because GSN stations
today commonly have more than one sensor installed SEED data commonly contain a loc
code to sort out which sensor is which.  The above shows the StationChannelMap description
for GSN stations SDV.  The hierarchy of channels described here (in order of decreasing 
precedence) are:  B channels with loc code 00, B channels with loc code 10, H channels
with loc code 10, and H channels with loc code 20.  
.LP
To complete the full StationChanellMap description try to make the default as all
encompassing as possible.  Usually you can use a generic default to specify a large
fraction of the data.  Each more complicated example will need a different template
with the station name as the tag.  In a worst case you might use a different entry
for each station, but this should not normally be necessary as almost all experiments
or networks use some standard channel naming conventions. 
.LP
\fIbeam_arrival_error_marker_color\fP and \fIbeam_arrival_pick_marker_color\fP
can be used to changed the default colors for the lines marking the pick time
for the beam trace and the interval for the beam arrival time error respectively.
.LP
\fIbeam_dfile\fP and \fIbeam_directory\fP control the file name and directory 
in which beam traces are written.  Beam data is written as raw binary doubles
in this directory/file location.  Each new beam is simply appended. 
.LP
\fIcoherence_cutoff\fP,
\fIcorrelation_peak_cutoff\fP, 
\fIstack_weight_cutoff\fP, and
\fItime_lag_cutoff\fP are all automatic cutoff on these four computed attributes.
(Note the first three can be plotted as attribute plots with the View menu.)
The names imply what attribute is involved: coherence, peak of cross correlation,
robust weight, and computed lag respectively.  In interactive use all four of 
these parameters should normally be left as the defaults.  The defaults 
are small positive numbers or the first three.  For time_lag_cutoff the 
number is the absolute value of the allowed range.  The default is plus
or minus 4 s, which is a reasonable upper bound.  Increasing this value will 
increase compute time as it also sets the range for which the cross-correlation 
functions are computed.
.LP
\fIdata_window_start\fP and \fIdata_window_end\fP define the time range around
the theoretical time the program tries to read.  It should be larger than the
range defined by regular_gather_start to regular_gather_end plus two times the
tpad value.  If resampling is required it would also be advised to make this 
window large enough to not be influenced by edge transients from the 
decimation FIR filters used in the resampling operators.  You will need to 
look at the response files used for decimators to provide an accurate estimate
of the amount of padding that will be required to avoid decimation artifacts.
You can request a large window, but you will pay by waiting longer
every time you read data if 
this window is much larger than necessary.  Note this, like the other time
window parameters used in this program, have units of seconds. 
.LP
\fIfilters\fP is used to define filters you want to use in processing.
This parameter is a tag for a Tbl of arbitrary length.  Each line of
the Tbl section linked to this tag defines one filter option.  
The format is best understood by an example:
.nf
filters &Tbl{
  telebb  BW 0.01 5 2 5
  lp      BW 0.025 5 0.08 5
  sp      BW 0.5 5 1.5 5
  integration     INT
}
.fi
.LP
The first token in each line (telebb, lp, sp, and integration in
the example above) is the label for this filter posted on the 
filter options radio box in the GUI).  The second token in each line
must be a valid filter description as described in trfilter(3) or
wffil(3).
.ce
\fIPlot Parameters\fP
.fi
.LP
There are three seismic plots made by this program: 
(1) the main data display, (2) a plot of the array beam, and
(3) a plot of the cross-correlation functions.  All three are
created with the same Motif widget.  The parameters that control
the display in each of these three 
windows are described in detail in seisw(3).  For this program
you need to realize that each of the three plots set individual 
parameters nested inside three &Arr with tags:  
data_window_parameters,
beam_window_parameters, and coherence_window_parameters.  
As the names imply these control the data display, beam, and
cross-correlation windows respectively.
.ce
\fITemplates\fP
.fi
.LP
The stock parameter file is designed for teleseismic body wave processing.  The contrib
distribution has two alternative parameter files called dbxcor_Plocal.pf and dbxcor_Slocal.pf
that contain recipes that can be used as a starting point for local event (source array) processing.
.SH DIAGNOSTICS
.LP
The log window is used to give feedback on progress of the analysis
and some errors.  The program can die on data problems that will
normally leave a diagnostic message on stderr.
.SH "BUGS AND CAVEATS"
.IP (1)
The beam and cross-correlation plots are not automatically destroyed
when a new event is loaded.  For now these must be manually closed to 
avoid confusion.  Both displays should probably also have a dismiss
button, but I've been able to make that work correctly so it remains to 
be implemented.
.IP (2) 
Be warned that dbcrunch is called on assoc and arrival when the program
exits.  Since this is intrinsically dangerous, backup you databases often
when processing with this program.
.IP (3)
The creation of two different output tables (xcorarrival and xsaa) depending
on the processing mode is problematic and at some point they will likely be 
merged into one table that can work on both modes.
.IP (4)
The informational text window does not refresh properly.  Whenever a message is posted
to that window the text is pushed to a line above the scroll bar and is not visible 
until the user hits the scrollbar widget to shift the text downward.
.IP (5)
When running on a database for event processing the event and origin tables 
must exist and be consistent.  This situation produces this error message 
that is easily misunderstood:
.nf
DatascopeMatchHandle constructor:  Input view does not contain a table containing the attribute orid
.fi
.SH AUTHOR
Peng Wang (pewang@indiana.edu) wrote the GUI.  The seismic analysis sections
were written by Gary Pavlis (pavlis@indiana.edu).  Later versions of the gui 
were extensively reworked by Pavlis.
.\" $Id$
